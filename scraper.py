import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from helpers import cvss_color_calc, cve_status_color_calc
from constants import search_urls


def scrape_nvd(cve_id):
    url = search_urls["NVDNIST"]
    response = requests.get(url + cve_id, verify=False)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")

        # Check for CVE ID Not Found
        # if soup.find("div", {"id": "ErrorPanel"}):
        #     cve_data = {"error": f"{cve_id} Not Found"}
        #     return cve_data

        # Extract the vulnerability description
        description = soup.find("p", {"data-testid": "vuln-description"})
        description = (
            description.text.strip() if description else "No description found"
        )

        cvss_score = soup.find("a", {"data-testid": "vuln-cvss3-panel-score"})
        cvss_score = cvss_score.text.strip() if cvss_score else "N/A"

        cvss_color = cvss_color_calc(cvss_score)
        cvss_vector = soup.find("span", {"data-testid": "vuln-cvss3-nist-vector"})
        cvss_vector = cvss_vector.text.strip() if cvss_vector else "N/A"

        published_on = soup.find("span", {"data-testid": "vuln-published-on"})
        published_on = published_on.text.strip() if published_on else "N/A"

        last_modified = soup.find("span", {"data-testid": "vuln-last-modified-on"})
        last_modified = last_modified.text.strip() if last_modified else "N/A"

        vuln_source = soup.find(
            "span", {"data-testid": "vuln-current-description-source"}
        )
        vuln_source = vuln_source.text.strip() if vuln_source else "N/A"

        cve_data = {
            "id": cve_id.upper(),
            "description": description,
            "cvss_score": cvss_score,
            "cvss_color": cvss_color,
            "cvss_vector": cvss_vector,
            "source": url + cve_id,
            "source_label": "NVD NIST",
            "published_on": published_on,
            "last_modified": last_modified,
            "vuln_source": vuln_source,
        }
    else:
        cve_data = {
            "error": "Failed to retrieve CVE data. Status code: {}".format(
                response.status_code
            )
        }

    return cve_data


def scrape_cveorg(cve_id):
    # Set up Selenium options
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode (no GUI)
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")

    # Initialize WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    try:
        url = search_urls["CVEOrg"]
        driver.get(url + cve_id)

        # Wait for the element to load
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "cve-state-tag"))
        )

        # Scrape the dynamically loaded content
        status_element = driver.find_element(By.CLASS_NAME, "cve-state-tag")
        status = status_element.text

        cve_data = {"status": status, "status_color": cve_status_color_calc(status)}
    except Exception as e:
        cve_data = {"error": str(e)}
    finally:
        driver.quit()

    return cve_data


def scrape_cvedetails(cve_id):
    url = search_urls["CVEDetails"]
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    response = requests.get(url + cve_id, headers=headers, verify=False)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")

        # Select the ul element and then get the li elements within it
        ul_element = soup.select_one("#contentdiv > div > main > div:nth-child(5) > ul")
        msf_modules_items = (
            ul_element.select("li.list-group-item") if ul_element else []
        )

        msf_modules = []

        for item in msf_modules_items:
            # Extract title with icon and description
            title_element = item.select_one("h5")
            title = title_element.get_text(strip=True) if title_element else None

            if title and title != "No title found":
                # Extract disclosure date and first seen date
                date_elements = item.select("div.ms-2")
                disclosure_date = "No disclosure date found"
                first_seen = "No first seen date found"

                if len(date_elements) > 0:
                    disclosure_date = (
                        date_elements[0]
                        .get_text(strip=True)
                        .replace("Disclosure Date: ", "")
                    )

                if len(date_elements) > 1:
                    first_seen = (
                        date_elements[1]
                        .get_text(strip=True)
                        .replace("First seen: ", "")
                    )

                # Extract module name
                module_name_element = item.select_one("div.ssc-text-secondary")
                module_name = (
                    module_name_element.get_text(strip=True)
                    if module_name_element
                    else "No module name found"
                )

                # Extract "More information" link
                more_info_element = item.select_one("a.ssc-ext-link")
                more_info_url = (
                    more_info_element["href"] if more_info_element else "No link found"
                )

                msf_modules.append(
                    {
                        "title": title,
                        "disclosure_date": disclosure_date,
                        "first_seen": first_seen,
                        "module_name": module_name,
                        "more_info_url": more_info_url,
                    }
                )

        cve_data = {"msf_modules": msf_modules}
    else:
        cve_data = {
            "error": "Failed to retrieve CVE data. Status code: {}".format(
                response.status_code
            )
        }
    return cve_data

def scrape_exploitdb(cve_id):
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")

    driver = webdriver.Chrome(options=chrome_options)

    # Scrape exploitdb for exploits